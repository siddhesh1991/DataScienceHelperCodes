{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n",
    "y = np.array([[0,1,1,0]]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [0, 1, 1],\n",
       "       [1, 0, 1],\n",
       "       [1, 1, 1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "syn0 = 2*np.random.random((3,4)) - 1\n",
    "syn1 = 2*np.random.random((4,1)) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.87560397,  0.97440574, -0.02816065, -0.10306781],\n",
       "       [-0.77211275,  0.5681985 , -0.88812913, -0.8911695 ],\n",
       "       [-0.28865131, -0.01255446,  0.12598176,  0.24277991]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.72607692],\n",
       "       [-0.73595849],\n",
       "       [ 0.85218147],\n",
       "       [ 0.67803997]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in xrange(60000):\n",
    "    l1 = 1/(1+np.exp(-(np.dot(X,syn0))))\n",
    "    l2 = 1/(1+np.exp(-(np.dot(l1,syn1))))\n",
    "    l2_delta = (y - l2)*(l2*(1-l2))\n",
    "    l1_delta = l2_delta.dot(syn1.T) * (l1 * (1-l1))\n",
    "    syn1 += l1.T.dot(l2_delta)\n",
    "    syn0 += X.T.dot(l1_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlin(x,deriv=False):\n",
    "    if(deriv == True):\n",
    "        return x*(1-x) #x is sigmoid output itself s(x)*(1-s(x))\n",
    "        #If the sigmoid's output is a variable \"out\", then the derivative is simply out * (1-out)\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[0,0,1],\n",
    "              [0,1,1],\n",
    "              [1,0,1],\n",
    "              [1,1,1] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5       ,  0.5       ,  0.73105858],\n",
       "       [ 0.5       ,  0.73105858,  0.73105858],\n",
       "       [ 0.73105858,  0.5       ,  0.73105858],\n",
       "       [ 0.73105858,  0.73105858,  0.73105858]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonlin(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.array([[0,0,1,1]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "syn0 = 2*np.random.random((3,1)) - 1\n",
    "#it's a good idea to have a mean of zero in weight initialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.16595599],\n",
       "       [ 0.44064899],\n",
       "       [-0.99977125]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output After Training:\n",
      "[[ 0.00274054]\n",
      " [ 0.00223536]\n",
      " [ 0.99817573]\n",
      " [ 0.99776324]]\n"
     ]
    }
   ],
   "source": [
    "for iter in xrange(10000):\n",
    "    #forward propogation\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0,syn0)) #if I add new layer this output will act as input and will also be a new feature\n",
    "    \n",
    "    # how much did we miss?\n",
    "    l1_error = y - l1\n",
    "    \n",
    "    # multiply how much we missed by the\n",
    "    # slope of the sigmoid at the values in l1\n",
    "    #The Error Weighted Derivative\n",
    "    #Weight the error with derivative of predicted values to improve weights\n",
    "    #Better ways available\n",
    "    l1_delta = l1_error * nonlin(l1,True)\n",
    "    \n",
    "    #update weights\n",
    "    #Transpose Data 3x4 and dot with l1_delta 4x1 to get new weights\n",
    "    #back propogate error , update weight per observation\n",
    "    #It computes the weight updates for each weight for each training example, sums them, and updates the weights, all in a simple line\n",
    "    syn0 += np.dot(l0.T,l1_delta)\n",
    "    \n",
    "print \"Output After Training:\"\n",
    "print l1  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### What the training below is going to do is amplify that correlation. It's both going to update syn1 to map it to the output, and update syn0 to be better at producing it from the input!\n",
    "##### Adding layers to neural network creates more features which are correlated to output or feature combined together produce output which is highly correlated to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[0,0,1],\n",
    "[0,1,1],\n",
    "[1,0,1],\n",
    "[1,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.array([[0],\n",
    "[1],\n",
    "[1],\n",
    "[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# randomly initialize our weights with mean 0\n",
    "syn0 = 2*np.random.random((3,4)) - 1 #4 set of weights for 3 input features as next layer has 4 neurons, this 4 neuron become new kind of feature\n",
    "#, i can also create 5,6, this will in turn generae a whole new data/hidden features for processing\n",
    "syn1 = 2*np.random.random((4,1)) - 1 #can increases this set of weights too but have to add next layer as output is a vector so last layer should have one set of weights, here 4 weights are given as output of previous layer has 4 hidden feature nothing to do with 4 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j in xrange(60000):\n",
    "\n",
    "    # Feed forward through layers 0, 1, and 2\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0,syn0))\n",
    "    l2 = nonlin(np.dot(l1,syn1))\n",
    "\n",
    "    # how much did we miss the target value?\n",
    "    l2_error = y - l2\n",
    "    \n",
    "    if (j% 10000) == 0:\n",
    "        print \"Error:\" + str(np.mean(np.abs(l2_error)))\n",
    "        \n",
    "    # in what direction is the target value?\n",
    "    # were we really sure? if so, don't change too much. thats why derivative it will calculate the delta to change the weight\n",
    "    l2_delta = l2_error*nonlin(l2,deriv=True)\n",
    "\n",
    "    # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "    #back propogation\n",
    "    l1_error = l2_delta.dot(syn1.T)#4x1 . 1x4 = 4x4 propogate error for previous layer\n",
    "    \n",
    "    # in what direction is the target l1?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    l1_delta = l1_error * nonlin(l1,deriv=True)\n",
    "\n",
    "    syn1 += l1.T.dot(l2_delta)\n",
    "    syn0 += l0.T.dot(l1_delta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Line 43: uses the \"confidence weighted error\" from l2 to establish an error for l1. To do this, it simply sends the error across the weights from l2 to l1. This gives what you could call a \"contribution weighted error\" because we learn how much each node value in l1 \"contributed\" to the error in l2. This step is called \"backpropagating\" and is the namesake of the algorithm. We then update syn0 using the same steps we did in the 2 layer implementation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
